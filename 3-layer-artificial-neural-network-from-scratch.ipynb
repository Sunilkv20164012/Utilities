{"cells":[{"metadata":{"_uuid":"be338c1d16074ccabc134910893e3f80da3dd1fb"},"cell_type":"markdown","source":"* For more detailed explanations, you can check the following link: https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners.\n* In this link, Artificial Neural Network (ANN) is explained in very detail and a 2-Layer ANN model is coded explicitly.\n* In my kernel, I implemented a 3-layer ANN model explicitly."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt # for vizualization\nfrom matplotlib.pyplot import figure # for figuresize","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90a844904bed69e10300ce6a1ebe335acde66fdd"},"cell_type":"markdown","source":"## Import data"},{"metadata":{"trusted":true,"_uuid":"56cb9a1dfafadf1d64a11823a30af8764b91b0a6"},"cell_type":"code","source":"voice_data = pd.read_csv(\"../input/voice.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6076bdb43406651943b5d441869f3486b846f594"},"cell_type":"markdown","source":"## Preview data"},{"metadata":{"trusted":true,"_uuid":"8e54afaa74d527bd6a8f62c40f816508257be013"},"cell_type":"code","source":"voice_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67d0acbf744488b106c1cdad2c7c37dae7e2df8e"},"cell_type":"markdown","source":"## Convert male to 1, female to 0"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"voice_data.label = [1 if each == \"male\" else 0 for each in voice_data.label]\nvoice_data.head() # check if 1-0 conversion worked","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51cb90e79a5cfb2a3d4fc3285d005d047af7c712"},"cell_type":"markdown","source":"## Extract gender (Y_data) and feature (X_data) information"},{"metadata":{"trusted":true,"_uuid":"b221033582e48e13103aeb93fd5e8c52a13d5289"},"cell_type":"code","source":"Y_data = voice_data.label.values\nX_data = voice_data.drop([\"label\"], axis = 1)\nX_data = (X_data - X_data.min())/(X_data.max() - X_data.min()) # normalization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42dcfef6e7a0ee790df695c077bd6699f431937f"},"cell_type":"code","source":"print(voice_data.shape)\nprint(X_data.shape)\nprint(Y_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30e52cdb9bad8aa092905b09135f8782b06777fe"},"cell_type":"markdown","source":"## Vizualization of voice_data, X_data and Y_data\n<a href=\"https://ibb.co/ePbiaA\"><img src=\"https://preview.ibb.co/gwBGvA/im1.png\" alt=\"im1\" border=\"0\"></a>"},{"metadata":{"_uuid":"b68a9c5d5893ca33fc7d300d8e693c6c0cadc1f9"},"cell_type":"markdown","source":"## Split data for train and test purposes"},{"metadata":{"trusted":true,"_uuid":"7982404a173bf826fbcc5dffc9bee6a4c5d90240"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X_data, Y_data, test_size = 0.3, random_state = 42)\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98d1170abd954741b878a6b5851bb1c2a9e3f818"},"cell_type":"code","source":"x_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc5804e20850bc3017bdbd00559f5110364fc588"},"cell_type":"code","source":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02a8dff346942454e06b1916326c6cfe13632523"},"cell_type":"markdown","source":"## 3-Layer Artificial Neural Network (ANN) Construction\n<a href=\"https://ibb.co/dGQUyV\"><img src=\"https://preview.ibb.co/eRs9yV/im2.png\" alt=\"im2\" border=\"0\"></a>"},{"metadata":{"_uuid":"6f351599cccbc4d4078e0b9cd70f0996d054ea87"},"cell_type":"markdown","source":"## Create weight and bias for 3-layer neural network ( 2 hidden layers )"},{"metadata":{"trusted":true,"_uuid":"11a450ef26597aa728b8fe73df7b804383ce73aa"},"cell_type":"code","source":"def initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n    \n    parameters = {\"W1\": np.random.randn(2,x_train.shape[0]) * 0.1,\n                  \"b1\": np.zeros((2,1)),\n                  \"W2\": np.random.randn(2,2) * 0.1,\n                  \"b2\": np.zeros((2,1)),\n                  \"W3\": np.random.randn(1,2) * 0.1,\n                  \"b3\": np.zeros((1,1))}\n    \n    return parameters\n\nparameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcee4137ccbe41f274f5d25e0b8804ffa625b34d"},"cell_type":"code","source":"print(parameters[\"W1\"].shape)\nprint(parameters[\"W2\"].shape)\nprint(parameters[\"W3\"].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11a1fffdd57f6a28b95c072068a7b3c7f3e55b5c"},"cell_type":"code","source":"print(parameters[\"b1\"].shape)\nprint(parameters[\"b2\"].shape)\nprint(parameters[\"b3\"].shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f945e9996f8d88d32a7080be804b76c67e7f482b"},"cell_type":"markdown","source":"## Sigmoid function :  sigmoid(x) = 1 / ( 1 + exp(-x) )"},{"metadata":{"trusted":true,"_uuid":"7225ca7f92dd454bb05b04f318a4c3ef47976aec"},"cell_type":"code","source":"def sigmoid(z):\n    return 1/(1 + np.exp(-z))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85bc50c95d2c28c6ce8f479df567589244ad8f48"},"cell_type":"markdown","source":"## Forward Propagation"},{"metadata":{"trusted":true,"_uuid":"fe949e0387cb25f689e723782e6b3906c4e6e868"},"cell_type":"code","source":"def forward_propagation_NN(x_train, parameters):\n    \n    Z1 = np.dot(parameters[\"W1\"],x_train) + parameters[\"b1\"]\n    A1 = np.tanh(Z1) # tanh is used as activation function 1\n    Z2 = np.dot(parameters[\"W2\"],A1) + parameters[\"b2\"]\n    A2 = np.tanh(Z2) # tanh is used as activation function 2\n    Z3 = np.dot(parameters[\"W3\"],A2) + parameters[\"b3\"]\n    A3 = sigmoid(Z3)\n\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"W1\": parameters[\"W1\"],\n             \"Z2\": Z2,\n             \"A2\": A2,\n             \"W2\": parameters[\"W2\"],\n             \"Z3\": Z3,\n             \"A3\": A3,\n             \"W3\": parameters[\"W3\"]}\n    \n    return A3, cache\n\nA3, cache = forward_propagation_NN(x_train, parameters)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c56374f85fa97ebc8f384133ce256c58ef9a71de"},"cell_type":"markdown","source":"## Loss function and Cost function\n* Loss and cost functions are same with logistic regression\n* Cross entropy function\n<a href=\"https://imgbb.com/\"><img src=\"https://image.ibb.co/nyR9LU/as.jpg\" alt=\"as\" border=\"0\"></a><br />"},{"metadata":{"_uuid":"0f874fa3471342415a0dc0064608d7975067dde6"},"cell_type":"markdown","source":"## Compute Cost"},{"metadata":{"trusted":true,"_uuid":"f75e7cb67cec5e030715f62f71b6fa055e812e01"},"cell_type":"code","source":"def compute_cost_NN(A3, Y, parameters):\n    \n    logprobs = np.multiply(np.log(A3),Y)\n    cost = -np.sum(logprobs)/Y.shape[1]\n    \n    return cost\n\ncost = compute_cost_NN(A3, y_train, parameters)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19eebf50636c31a1c0dde016f49c196475024607"},"cell_type":"markdown","source":"## Backward Propagation with Gradient Decent"},{"metadata":{"trusted":true,"_uuid":"22f3787645ca51abe55c7bccc2a783f7b7bcd1b9"},"cell_type":"code","source":"def backward_propagation_NN(parameters, cache, X, Y):\n    \n    dimension = X.shape[0] # it is 20 for our case\n    dZ3 = cache[\"A3\"] - Y # d(cost)/d(Z3)\n    dW3 = 1/dimension * np.dot(dZ3,cache[\"A2\"].T) # d(cost)/d(W3)\n    db3 = 1/dimension * np.sum(dZ3, axis=1, keepdims=True) # d(cost)/d(b3)\n    dZ2 = np.multiply(np.dot(dZ3.T, cache[\"W3\"]).T , 1-np.power(cache[\"A2\"],2)) # d(cost)/d(Z2)\n    dW2 = 1/dimension * np.dot(cache[\"A1\"], dZ2.T) # d(cost)/d(W2)\n    db2 = 1/dimension * np.sum(dZ2, axis=1, keepdims=True) # d(cost)/d(b2)\n    dZ1 = np.multiply(np.dot(dZ2.T, cache[\"W2\"].T).T,1-np.power(cache[\"A1\"],2)) # d(cost)/d(Z1)\n    dW1 = 1/dimension * np.dot(dZ1, X.T) # d(cost)/d(W1)\n    db1 = 1/dimension * np.sum(dZ1,axis=1, keepdims=True) # d(cost)/d(b1)\n    grads = {'dW3':dW3, \n             'db3':db3,\n             'dW2':dW2,\n             'db2':db2,\n             'dW1':dW1,\n             'db1':db1}\n    \n    return grads\n\ngrads = backward_propagation_NN(parameters, cache, x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06e598a64281f7d43cc9d9cf60f432f830e179c6"},"cell_type":"markdown","source":"## Set the learning rate"},{"metadata":{"trusted":true,"_uuid":"d8832af043584b569157e1e291362ff41c9b12a3"},"cell_type":"code","source":"Learning_Rate = 0.001","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecdc8b9fa164ecec1f884b28caa918caf2bf0879"},"cell_type":"markdown","source":"## Update weight and bias"},{"metadata":{"trusted":true,"_uuid":"39b64d274b1d3acd4ae40163c534e2b4ecd25178"},"cell_type":"code","source":"def update_parameters_NN(parameters, grads, learning_rate = Learning_Rate):\n    parameters = {\"W1\": parameters[\"W1\"]-learning_rate*grads[\"dW1\"],\n                  \"b1\": parameters[\"b1\"]-learning_rate*grads[\"db1\"],\n                  \"W2\": parameters[\"W2\"]-learning_rate*grads[\"dW2\"],\n                  \"b2\": parameters[\"b2\"]-learning_rate*grads[\"db2\"],\n                  \"W3\": parameters[\"W3\"]-learning_rate*grads[\"dW3\"],\n                  \"b3\": parameters[\"b3\"]-learning_rate*grads[\"db3\"]}\n    \n    return parameters\n\nparameters = update_parameters_NN(parameters, grads, learning_rate = Learning_Rate)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"397537a1aa09ed7055d7ecac9a7985dd962532ad"},"cell_type":"markdown","source":"## Predict for the test data with updated weight and bias (with updated parameters)"},{"metadata":{"trusted":true,"_uuid":"37fd0a27d0d4e761952b8a82f2f05955f922515f"},"cell_type":"code","source":"def predict_NN(parameters,x_test):\n    # x_test is the input for forward propagation\n    A3, cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n\n    for i in range(A3.shape[1]):\n        if A3[0,i]<= 0.5: # if smaller than 0.5, predict it as 0\n            Y_prediction[0,i] = 0\n        else: # if greater than 0.5, predict it as 1\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n\nY_prediction = predict_NN(parameters,x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2eaf6cafedf7787130745b7db5391c66a74548f2"},"cell_type":"markdown","source":"## Create 3-layer Neural Network Model"},{"metadata":{"trusted":true,"_uuid":"ef4cbb3b1d48b9a4d08101dbe9d4682f8c924b49"},"cell_type":"code","source":"def three_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n    cost_list = []\n    index_list = []\n    \n    #initialize parameters and layer sizes\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n\n    for i in range(0, num_iterations):\n        # forward propagation\n        A3, cache = forward_propagation_NN(x_train,parameters)\n        # compute cost\n        cost = compute_cost_NN(A3, y_train, parameters)\n        # backward propagation\n        grads = backward_propagation_NN(parameters, cache, x_train, y_train)\n        # update parameters\n        parameters = update_parameters_NN(parameters, grads)\n        \n        if i % 100 == 0: # to visualize data in each 100 iteration\n            cost_list.append(cost)\n            index_list.append(i)\n\n    figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarions\", fontsize = 14)\n    plt.ylabel(\"Cost\", fontsize = 14)\n    plt.show()\n    \n    # predict\n    y_prediction_test = predict_NN(parameters,x_test)\n    y_prediction_train = predict_NN(parameters,x_train)\n\n    # Print train/test Accuracies\n    print(\"train accuracy: %{}\".format(round(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100,3)))\n    print(\"test accuracy: %{}\".format(round(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100,3)))\n    return parameters\n\nparameters = three_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=5000)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}